---
title: "Tax: A fully sharded data parallel trainer in jax"
date: 2024-04-14
# layout: default_posts
---
<!-- <div style="max-width: 400px" >
<img src="image.jpg" style="max-width:100%;" />
</div> -->

<figure style="text-align: center;">
  <img
     src="{{'/assets/img/jax_logo_250px.png' | relative_url }}"
     alt="jax"
     class="img-responsive"
     style="float: center; width:50%; margin-left: 25%;"
    >
  <!--figcaption>MDN Logo</figcaption   -->
</figure>

JAX is a powerful framework that I frequently use in my projects. Its features, like the `scan` function, the ease of calculating gradients, and the ability to run code on both TPUs and GPUs, are significant advantages. However, it isn't as mature as PyTorch in some areas.

In my research, I needed a training framework that could handle medium-sized models (around 2 billion parameters), so I decided to develop a fully sharded data-parallel trainer. While the primary focus is on research rather than production code, I thought it might be helpful to share it as a library.

You can check out the library [here](https://github.com/raresdolga/tax). I hope others find it useful too! Please star and cite if you use it.