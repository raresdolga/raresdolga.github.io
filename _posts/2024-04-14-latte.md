---
title: "Latte: Latent Attention for Linear Time Transformers"
date: 2024-04-14
# layout: default_posts
---
<!-- <div style="max-width: 400px" >
<img src="image.jpg" style="max-width:100%;" />
</div> -->

<figure style="text-align: center;">
  <img
     src="{{'/assets/img/latte.png' | relative_url }}"
     alt="bid_late"
     class="img-responsive"
     style="float: center; width:50%; margin-left: 25%;"
    >
  <!--figcaption>MDN Logo</figcaption   -->
</figure>

Transformers are one of the most popular architectures used in both sequence modelling and computer vision. At the centre of Transformers is the attention mechanism which compares each element of a sequence with every other element. This pairwise similarity score is used to decide how much the other tokens contribute to the new representation of one element. While the approach gives state-of-the-art results, it comes at the cost of quadratic time complexity. Additionally, for language generation, the next token prediction is linear in the prompt length, compared to the constant time complexity of approaches like Structured State Models (SSMs)[^1].

We introduce Latte[^2], a new linear time and memory replacement for standard attention, which achieves a comparable performance to Transformers while being more efficient during training and inference. These properties are important for document modelling or high-resolution visual question answering, where input can be very long. In this blog post, we focus on an intuitive explanation of Latte, but the approach is inspired and can be easily understood from the lens of latent variables. For a concise mathematical explanation, check out <a href="https://arxiv.org/abs/2402.17512" target="_blank">our paper</a>.

We will first rewrite the classic attention mechanism in the non-vectorized form, which will help us describe the idea behind Latte.

<h2>Quick Recap</h2>

One of the most common ways of writing a standard attention layer is using the matrix form[^4]:
 \[
 \text{Softmax} \bigg ( \frac{1}{\sqrt{d}}QK^\mathsf{T} \bigg )V
 \]

Nonetheless, bearing in mind that standard attention is based on pairwise interactions between elements of a sequence, the formula can be written more intuitively without any vectorization. Defining a sequence of tokens $x_{1:T}$ from which we obtain the queries $q_t$, keys $k_t$ and values $v_t$ we explain standard attention in Figure 1. Mathematically, one can write the above for the new \\({\tilde{x}}\_{t}\\) vector:
\\(a_{ts} = \frac{\exp(q_t^\mathsf{T}k_s)}{\sum_{s=1}^T \exp(q_t^\mathsf{T}k_s) } := p(s|t) \\)
and
\begin{aligned} {\tilde{x}}\_{t} &= \sum_{s=1}^T a_{ts}v_s \\\ &= \sum_{s=1}^T p(s|t) v_s \end{aligned}

<figure style="text-align: center;">
  <img
     src="{{'/assets/img/att_sum.png' | relative_url }}"
     alt="Attentention Non-Vect"
     class="img-responsive"
     style="float: center;"
    >
    <br>
  <figcaption>Figure1: Non-vectorized standard attention mechanism</figcaption>
</figure>

Hence, the new representation of ${\tilde{x}}\_{t}$ is a combination of all elements, weighted by their similarity with the current token $x_t$. To make the connection with the matrix form, we observe that $q_t^\mathsf{T} = Q_{t,:}$ and $k_s = K^\mathsf{T}_{:,s}$. Notice that we can think of $a _{ts} $ as the probability of occurrence for a token at position $ s $ given the token at position $ t $. This observation will help us understand our latent approach. Here we defined bidirectional standard attention, but for the causal case, we would sum up to index $t$ instead of the entire sequence length $T$. This ensures that the new representation does not incorporate tokens in the future which are not available at test time.

<h2>Bidirectional Latte</h2>
As previously stated, the bottleneck of standard attention is computing weights $a_{ts}$. We mitigate this by introducing learnable latent tokens that are compared to each element of the sequence. Since the number of latent tokens is fixed, the computation cost becomes linear in the sequence length. Intuitively, we can think of the latent variables as concepts like colours or shapes to which we compare the input. Then our method creates a new representation using all the sequence tokens and their similarity to the learned high-level concepts. In Figure 2, we show the difference between bidirectional Latte and bidirectional standard attention metods.

<figure style="text-align: center;">
  <img
     src="{{'/assets/img/latte_graph.png' | relative_url }}"
     alt="Comparison Attentions"
     class="img-responsive"
     style="float: center;"
    >
  <figcaption>Figure 2: Comparison between Bidirectional Standard Attention (Left) and Sparse Attention and Bidirectional Latte(Right).</figcaption>
</figure>

The approach has similarities with sparse attention methods such as BigBird[^3], which only compute attention between a set of learnable global tokens and all the sequence elements. However, the main difference is that the sparse methods are weighted sums of the global tokens, while in our approach we consider the entire sequence. Specifically, we define a different parametrisation of full attention using latent variables, instead of only performing attention between the latents and the sequence elements. 

Defining our previous observation that attention has a probabilistic interpretation, we can re-parameterise $a_{ts}$ with a weighted sum based on $L$ learnable latent variables:
\begin{aligned} a_{ts} = p(s|t) &= \sum_{l=1}^L p(s,l|t) \\\ &= \sum_{l=1}^L p(l|t) p(s|l) \end{aligned}
In the above, we assumed independence between $ s $ and $ t $ give $ l $. Intuitively, we compute the similarity between a high-level concept and each element, then we re-weight it based on the similarity between our current token and the high-level concepts. These concepts are our latent variables which we learn end to end in tasks like classification or language modelling. Hence, they might not necessarily be interpretable. To calculate the probabilities above, we can reuse the attention matrices $ Q $ and $ K $, giving us the new vector representation of each token:
\[
\tilde{X} = \text{Softmax} ( Q ) \text{Softmax}(K)^\mathsf{T} V
\]

Note that $Q$ and $K$ have different sizes than the queries and keys in the standard attention. Figure 3 describes in detail how we obtain these matrices.

<figure style="text-align: center;">
  <img
     src="{{'/assets/img/latte_block.png' | relative_url }}"
     alt="Latte Implementation"
     class="img-responsive"
     style="float: center;"
    >
    <br>
  <figcaption> Figure 3: Each row in $X$ corresponds to a token in the input sentence. We multiply the input by the corresponding weights and obtain $Q$, $K$, $V$. We exponentiate and normalize $Q$ on columns across all latents and $K$ on rows across the sequence. </figcaption>
</figure>

Our formulation results in $O(TLD)$ time and $O(TL + LD)$ memory complexity, compared to the $O(T^2D)$ of the standard bidirectional approach. We defined $ D $ to be the vector dimension. The approach is not entirely new. Other works have decomposed attention in the same fashion for the bidirectional case. However, our probabilistic framework easily allows us to extend our model to the causal case.

## Causal Latte
In the previous sections, we described the bidirectional case, but for problems like language generation, we need a causal mechanism. The change can be trivially seen by looking at the formula for $\tilde{x}\_t$ and only sum up to index $t$ instead of the entire sequence. This means that we have a cumulative sum and we cannot simply apply the softmax function over $K$. Instead, we need an approach which updates sequentially the normalisation factor and the weight given by $K$. Defining $Q^{\'} = \text{Softmax} ( Q ) $ and $K^{\'} = \exp(K)$ we can write
\[
\tilde{x}\_t = \sum_{l=1}^L Q_{tl}^{\'} \frac{1}{\sum_{s=1}^t K_{ls}^{\'}} \sum_{s=1}^t K_{ls}^{\'}V_{s,:}
\]

The formulation above can be vectorized. However, a sequential implementation has the benefit of constant time complexity for the next token prediction task. Hence, predicting $T$ tokens requires $O(TLD)$ time and $O(TL + LD)$ memory.

## Relative Embeddings
Relative embeddings generalise better to unseen sequence length when compared to additive positional embeddings. However, in their standard form, they do not make sense to be used for latent tokens. We therfore introduce VAPOR (VAlue embedded POsitional Rotations) which computes the relative distance between tokens, but without affecting the attention weights:
\[
\tilde{x}\_t =  \sum_{s=1}^t p(s|t) R^{t-s}v_s = R^t \sum_{s=1}^t p(s|t) R^{-s}v_s = R^t \sum_{l=1}^L p(l|t) \sum_{s=1}^t p(s|l,t) R^{-s}v_s
\]
## Results
### Runtime Efficiency
We developed a method with linear time and memory complexity in the sequence length. One drawback is that the causal version needs to be implemented sequentially to decrease memory usage and have constant time inference. If the sequence length is small, this can be slower than a vectorized version of standard attention on GPUs. To see the benefits of Latte, we perform an analysis of runtime performance in Figure 4.

<figure style="text-align: center;">
  <img
     src="{{'/assets/img/eff.png' | relative_url }}"
     alt="Efficiency"
     class="img-responsive"
     style="float: center;"
    >
    <br>
  <figcaption>Figure 4: Time and memory comparison of bidirectional and causal standard attention and Latte. For the former, we also test its sequential implementation with linear memory complexity. We set the number of latents to $L = 128$ while the rest of the hyperparameters are the same as the ones used in the standard attention.</figcaption>
</figure>

From the above, we can see that the bidirectional case is faster than the standard attention even when the sequence length is small. However, the sequential causal model has a better runtime performance than causal attention only for sequences longer than 3000 tokens. In terms of memory, Latte is more efficient even when the sequence has a smaller length. The results are dependent on the number of latent variables which give a tradeoff between the runtime efficiency and the complexity of a model.

### Long Range Arena
Long Range Arena is a synthetic benchmark which tests the ability of models to capture long-range dependencies on sequences of 2000 to 16000 tokens. All the tasks in the benchmark treat the input as a sequence of tokens and are formulated as classification problems[^5]. Consequently, the performance of the model is measured with accuracy, where a higher score means a better model.

We implement the tasks with a Bidirectional Latte model using 40 latents and show that we outperform the standard attention. The low number of latents results in a model which is faster than the standard attention, while still having better performance. We also compare Bidirectional Latte to other efficient Transformers and obtain comparable results, with the benefit that our method could easily be applied in both causal and bidirectional cases.

| Model | ListOps | Text | Retreival | Image | PathfInder|
|----------------| ---|---|---|--- | --- |
| Latte | 40.18 | 64.5 | 73.39 | 47.55 | 75.61 |
Standard Transformer | 36.37 | 64.27 | 57.46 | 42.44 | 71.40 |
Linformer | 35.70 | 53.94 | 52.27 | 38.56 | 76.34 |
| Luna | 38.01 | 65.74 | 79.55 | 47.47 | 78.89 |

Table1: Long Range Arena test set results for bidirectional layer. The higher the score the better. 

## Language Generation
For language modelling, we train a Causal Latte model on the next token prediction task. The datasets used are Wiki103, OpenWebtext and Enwik8. We tokenize the first two with a byte pair encoding tokenizer, while for the latter we used a character tokenizer. The sequence lengths are 1024 and 2048 for the two tokenization types. Two common metrics that we also use to measure the success of this task are perplexity (PPL) and bits-per-character (BPC). PPL is the exponential of the negative log-likelihood, meaning that a lower score indicates a better model. Similarly, BPC is the negative log-likelihood transformed in based two such that it indicates the number of bits used to represent a character. Again, a lower score means a better model.

We set the number of latent variables $L=128$ such that the model is faster than the standard causal attention while obtaining comparable results, as reported in Table 2.

| Model | Wiki103 (PPL) | OpenWebText (PPL)| Enwik-8 (BPC) |
|-------|---|--- |---|
| Vapor Causal Latte | 21.75 (152M) | 20.62 (150M) | 1.30 (36M) |
| Standard Causal Attention (Ours) | 21.81 (162M) | 20.51 (162M) | 1.16 (38M)
| Transformer-XL  |24.0 (151M) | - | 1.06 (41M) |

Table 2: Dev scores for causal language generation with 128 latent variables. The lower the scores the better the model.

<!-- On the Wiki103 text dataset, Latte outperforms other models by far. We also compare with Retro, to show that lower perplexities on Wiki103 are possible. However, it is important to note that Retro uses external information to generate text, such as billions of tokens extracted from the internet. Hence, it does not depend only on the user prompt like our model. In the case of more complex datasets like OpenWebText or character-level like Enwik8, our model does not beat the state of the art. This is expected since we have a tradeoff between the performance of the model and its computational complexity. -->
On token-level language modelling tasks, Latte combined with VAPOR obtains scores close to the standard attention. This is shown by experiments on Wiki103 and OpenWebText datasets. We also benchmark against Transformer-XL, a recursive model built for long sequences, and we get better results for a comparable number of parameters. While these results are promising considering the runtime gains, our model has some disadvanatges on character-level data sets like Enwik8. For this setting, patterns are more difficult to observe and elementwise interaction between characters might be required to increase performance. Nonetheless, the results show a tradeoff between computational complexity and model capacity. 

# Final Thoughts
Inspired by the fact that language can be decomposed into higher-level concepts, we developed a simple framework for bidirectional and causal cases that acts as a replacement for standard attention. Following the probabilistic interpretation, our model is easy to implement and has a fast and memory-effective runtime while it achieves better or comparable performance on classification and language generation tasks. Another benefit of our approach is that the next token prediction runs in constant time, resulting in a fast model during generation. Latte is a flexible model which we would also like to apply in multimodal tasks like visual question answering. Check out our <a href="https://github.com/raresdolga/latte_transformer" target="_blank">code</a> for more details!

### References
[^1]: Albert Gu, Karan Goel, and Christopher RÃ©. "Efficiently Modelling Long Sequences with Structured State Spaces." arXiv preprint arXiv:2111.00396 (2021).
[^2]: Rares Dolga, Marius Cobzarenco, and David Barber. "Latent Attention for Linear Time Transformers." arXiv Preprint arXiv:2402.17512 (2024).
[^3]: Manzil Zaheer, et al. "BigBird: Transformers for Longer Sequences." Advances in Neural Information Processing Systems 33 (2020)
[^4]: Ashish Vaswani, et al. "Attention Is All You Need." Advances in Neural Information Processing Systems 30 (2017).
[^5]: Yi Tay, et al. "Long Range Arena: A Benchmark for Efficient Transformers." arXiv Preprint arXiv:2011.04006 (2020).